{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Competition: House Prices\n",
    "\n",
    "Project Team: Jason Harville, Dan Wald, Andrea Pope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep, Data Load and Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# general libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy import *\n",
    "import pickle\n",
    "import math\n",
    "import csv\n",
    "from itertools import izip\n",
    "\n",
    "# functions to clean data and process data throughout, see external .py file\n",
    "from data_funcs import *\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#sklearn regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#sklearn classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UN-PICKLE FROM PREVIOUS RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results from previous runs, used to speed execution at different points through (avoid repeating long steps)\n",
    "# code used to produce pickle also included throughout\n",
    "model_results = pickle.load(open( \"./model_results.p\", \"rb\" ) )\n",
    "grid_results = pickle.load(open( \"./grid_results.p\", \"rb\" ) )\n",
    "best_cat_models = pickle.load(open(\"./best_cat_models.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing and Prep\n",
    "\n",
    "The data available was divided into 3 main types of data:\n",
    "1. Categorical: divided into dummy variables representing each sub-category\n",
    "2. Continuous: numerical data\n",
    "3. Likert scale: ratings converted to numerical scale\n",
    "\n",
    "After the raw processing of each data type, mean normlization was conducted to ensure no feature had undue weight in comparison to another feature.\n",
    "\n",
    "Additionally, 5 features were excluded due to insignificant population or information similar to all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import + Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code is intentionally commented out but included so that the reviewer understands how our team split the data. \\nAll development and evaluation has been performed on the split data included with the submission in the 'data' folder.\\n\\nDO NOT RUN.  If this block of code is executed, it will re-write the new train and dev sets randomly, \\nyeilding non-repeatable results\\n\\nThe code below imports the source file, shuffles, splits and exports the data as two seperate .csv files \\nlocated in ./data folder\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This code is intentionally commented out but included so that the reviewer understands how our team split the data. \n",
    "All development and evaluation has been performed on the split data included with the submission in the 'data' folder.\n",
    "\n",
    "DO NOT RUN.  If this block of code is executed, it will re-write the new train and dev sets randomly, \n",
    "yeilding non-repeatable results\n",
    "\n",
    "The code below imports the source file, shuffles, splits and exports the data as two seperate .csv files \n",
    "located in ./data folder\n",
    "'''\n",
    "# # importing original training dataset\n",
    "# train_original = pd.read_csv('data/train.csv')\n",
    "# num_examples = len(train_original)\n",
    "\n",
    "# # shuffling indices\n",
    "# shuffle = np.random.permutation(np.arange(num_examples))\n",
    "\n",
    "# # sorting training set by shuffled indices and splitting to training and dev sets\n",
    "# train_shuffled = train_original.iloc[shuffle]\n",
    "# train_split = train_shuffled[num_examples/10:]  \n",
    "# dev_split = train_shuffled[:num_examples/10]\n",
    "\n",
    "# # saving to csv\n",
    "# train_split.to_csv('./data/train_split.csv')\n",
    "# dev_split.to_csv('./data/dev_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the split data\n",
    "\n",
    "train = pd.read_csv('./data/train_split.csv')\n",
    "dev = pd.read_csv('./data/dev_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The below code references functions defined in data_funcs.py, included with submission\n",
    "'''\n",
    "\n",
    "# Variables for processing \n",
    "# Leverages early work to identify variable types that are handled seperately in transform_features function\n",
    "normal_cat_features = ['MSZoning','LandContour','LotShape','LotConfig','LandSlope','Neighborhood','BldgType','HouseStyle','RoofStyle','MasVnrType',\n",
    "                       'Foundation','Heating','Electrical','Functional','GarageType','GarageFinish','PavedDrive',\n",
    "                       'Fence','MiscFeature','SaleType','SaleCondition']\n",
    "\n",
    "special_cat_features = ['CentralAir','Condition1','Condition2','Exterior1st','Exterior2nd','BsmtFinType1','BsmtFinType2']\n",
    "\n",
    "used_cat_features = normal_cat_features + special_cat_features\n",
    "\n",
    "drop_features = ['MSSubClass','Street','Alley','Utilities','RoofMatl']\n",
    "drop_dummies = ['MSZoning_C (all)','LotShape_Reg','LotShape_IR1','LotShape_IR2','LotShape_IR3','LotConfig_FR2','LotConfig_FR3']\n",
    "\n",
    "feature_dict = get_train_cat_features(train_data=train, used_cat_features=used_cat_features)\n",
    "\n",
    "likert_features = ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','HeatingQC','KitchenQual',\n",
    "                   'FireplaceQu','GarageQual','GarageCond','PoolQC']\n",
    "\n",
    "# Getting labels and data\n",
    "train_labels = train['SalePrice']\n",
    "dev_labels = dev['SalePrice']\n",
    "\n",
    "# restructure the datasets to keep only variables and not the output\n",
    "keep_columns = [col for col in train.columns if col not in ['Unnamed: 0', 'SalePrice', 'Id']]\n",
    "train_data = train.set_index('Id')[keep_columns]\n",
    "dev_data = dev.set_index('Id')[keep_columns]\n",
    "\n",
    "#transforming features\n",
    "train_data = transform_features(train_data,feature_dict,normal_cat_features,used_cat_features,\n",
    "                                likert_features,drop_dummies,drop_features)\n",
    "dev_data = transform_features(dev_data,feature_dict,normal_cat_features,used_cat_features,\n",
    "                              likert_features,drop_dummies,drop_features)\n",
    "\n",
    "# Mean normalizing\n",
    "features_to_normalize = train_data.columns\n",
    "mn_values = {value: find_mu_std(train_data, value) for value in features_to_normalize}\n",
    "\n",
    "train_data = mean_normalization(train_data, features_to_normalize, mn_values)\n",
    "final_features = train_data.columns\n",
    "dev_data = mean_normalization(dev_data, features_to_normalize, mn_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "# convert to np arrays\n",
    "train_data = np.array(train_data)\n",
    "dev_data = np.array(dev_data)\n",
    "type(train_data)\n",
    "\n",
    "# deal with NaNs (which won't work in regression)\n",
    "where_are_NaNs = isnan(train_data)\n",
    "train_data[where_are_NaNs] = 0\n",
    "\n",
    "where_are_NaNs = isnan(dev_data)\n",
    "dev_data[where_are_NaNs] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable. These methods are simple to run and understand and are in general particularly good for gaining a better understanding of data (but not necessarily for optimizing the feature set for better generalization). There are lot of different options for univariate selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Across Different Categorical Thresholds\n",
    "\n",
    "Early threshold and parameter tuning.  The below code obtains the best categories.  Since this is only run once, there is no need to encapsulate as a function\n",
    "\n",
    "The best accuracy in the categorical division does not necessarily drive the best categorization for the final price prediction.\n",
    "For that reason, we'll take an approach that manually goes through various thresholds for splitting the training data, and\n",
    "feature pruning, and matching into categories.\n",
    "We'll run each of those through a separate pipeline (to be optimized) to measure accuracy of the final price prediction. Adjusting parameters in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Thresholds:  [350000]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this set of code is determining best way to split data into categories\n",
    "code runs an initial price prediction (to guage success of categories, but this will be enahnced later)\n",
    "\n",
    "NOTE: thresholds used were based on analysis of the original data, and our original baseline results\n",
    "The large set of test thresholds requires significant computing. \n",
    "We ran overnight and pickled the data for future use to prevent the need to re-process\n",
    "The current thresholds selected turned out to be the winning set, so a new user will arrive at the same result without \n",
    "significant processing.\n",
    "\n",
    "In addition t thresholds, other components have also been simplified to make processing easier (to prove code runs).\n",
    "However, the results in the pickle file were against the larger range of components.\n",
    "\n",
    "For data manipulation, we have pickled the data so that this code does not need to be run at all to arrive at conclusions.\n",
    "'''\n",
    "# test_thresholds = [[40000], [80000], [100000],[150000], [200000], [300000], [350000], [400000], [450000],\n",
    "#                  [40000, 300000], [40000,350000], [40000,400000],[80000, 300000], [80000,350000], [80000,400000]]\n",
    "# test_thresholds = [[40000,350000]]\n",
    "\n",
    "# C=[0.1, 0.5, 1, 1.5, 10]\n",
    "# estimators = [100, 500, 1000, 5000]\n",
    "# test_components = [5, 10, 15, 20, 25]\n",
    "test_thresholds = [[350000]]\n",
    "C=[10]\n",
    "estimators = [100]\n",
    "test_components = [10]\n",
    "\n",
    "model_results = []\n",
    "\n",
    "# determine how to break the data into categories\n",
    "for t in test_thresholds:\n",
    "    print 'Category Thresholds: ', t\n",
    "    train_cat_labels = list(categorize_prices(labels=train_labels, thresholds = t))\n",
    "    dev_cat_labels = list(categorize_prices(labels=dev_labels, thresholds = t))\n",
    "    \n",
    "    # L1 to build subset of data (without low value features)\n",
    "    train_subset, dev_subset, nonzero_features = run_logr_l1(train_data, train_cat_labels, dev_data, \n",
    "                                                             final_features)\n",
    "    # further reduction with PCA\n",
    "    # test withh several different # of components\n",
    "    for c in test_components:\n",
    "        if (c < train_subset.shape[1]):\n",
    "            # check we have that many components\n",
    "            train_pca_subset, dev_pca_subset = run_pca(train_subset, dev_subset, components = c)\n",
    "    \n",
    "            #Place into categories using logistics l2\n",
    "            for c1 in C:\n",
    "                train_categories, dev_categories = run_logr_l2(train_pca_subset, train_cat_labels, \n",
    "                                                               dev_pca_subset, c=c1)\n",
    "        \n",
    "                #run Linear Regression on each class defined and calculate overall accuracy\n",
    "                result = run_lr(train_categories, dev_categories, train_pca_subset, train_labels, dev_pca_subset, dev_labels)\n",
    "                model_results.append([t, c, \"logr\", c1, \"lr\", \"\", result])\n",
    "        \n",
    "                for e in estimators:\n",
    "                    result = run_rfr(train_categories, dev_categories, train_data, train_labels, dev_data, \n",
    "                                     dev_labels, estimators = e)\n",
    "                    model_results.append([t, c, \"logr\", c1, \"rfr\", e, result])\n",
    "\n",
    "            # Place into categories using Random Forrest Classifier\n",
    "            for e in estimators:\n",
    "                train_categories, dev_categories = run_rfc(train_pca_subset, train_cat_labels, \n",
    "                                                           dev_pca_subset, estimators=e)\n",
    "        \n",
    "                #run Linear Regression on each class defined and calculate overall accuracy\n",
    "                result = run_lr(train_categories, dev_categories, train_pca_subset, train_labels, dev_pca_subset, dev_labels)\n",
    "                model_results.append([t, c, \"rfc\", e, \"lr\", \"\", result])\n",
    "        \n",
    "                for e2 in estimators:\n",
    "                    result = run_rfr(train_categories, dev_categories, train_data, train_labels, dev_data, \n",
    "                                     dev_labels, estimators = e2)\n",
    "                    model_results.append([t, c, \"rfc\", e, \"rfr\", e2, result])\n",
    "\n",
    "            # Place into categories using adaboost decision tree\n",
    "            for e in estimators:\n",
    "                for lrn_rate in (0.1, 1):\n",
    "                #for lrn_rate in (0.1, 0.5, 1, 1.5):\n",
    "                    train_categories, dev_categories = run_abc(train_pca_subset, train_cat_labels, \n",
    "                                                               dev_pca_subset,\n",
    "                                                               estimators=e, lrn_rate=lrn_rate)\n",
    "                        \n",
    "                    #run Linear Regression on each class defined and calculate overall accuracy\n",
    "                    abc_desc = str(e) + \", \" +  str(lrn_rate) \n",
    "                    result = run_lr(train_categories, dev_categories, train_pca_subset, train_labels, dev_pca_subset, dev_labels)\n",
    "                        \n",
    "                    model_results.append([t, c, \"adc\", abc_desc, \"lr\", \"\", result])\n",
    "        \n",
    "                for e2 in estimators:\n",
    "                    result = run_rfr(train_categories, dev_categories, train_data, train_labels, dev_data, \n",
    "                                     dev_labels, estimators = e2)\n",
    "                    model_results.append([t, c, \"abc\", abc_desc, \"rfr\", e2, result])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The best Categorization breakdown and model to date\n",
    "\n",
    "Best ever result(stoichastic):\n",
    "[[40000, 300000, 'logr', 0.1441778680151591]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "If you run the code above, if you want to see results, similar to our team results (run on the larger data set),\n",
    "you can reload the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top three models \n",
      "[[[40000, 350000], 10, 'logr', 10, 'lr', '', 0.15368958570482674], [[40000, 350000], 25, 'logr', 1, 'lr', '', 0.15462395642721133], [[40000, 350000], 15, 'adc', '500, 3, 0.1', 'lr', '', 0.15523693930362142]]\n"
     ]
    }
   ],
   "source": [
    "# top 3 classifier model results\n",
    "model_results_sorted = sorted(model_results, key=lambda x: x[6])\n",
    "top_3_models = model_results_sorted[0:3]\n",
    "best_model = model_results_sorted[0]\n",
    "print 'top three models \\n', top_3_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with best results, rerun to break into categories based on threshold, method, and parameter indicatec\n",
    "possibly could run for top 3 best results (or top n)\n",
    "\n",
    "With the data separated into categories, we'll enhance the price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this is to determine best price predictin model per categry\n",
    "the best model for each category then separated to be run against test data\n",
    "split into categories as thresholds defined as best\n",
    "'''\n",
    "#redefine variables\n",
    "t = best_model[0] #threshold\n",
    "c = best_model[1] #PCA components\n",
    "c1 = best_model[3] #L2 regularization\n",
    "grid_results = []\n",
    "\n",
    "train_cat_labels = list(categorize_prices(labels=train_labels, thresholds = t))\n",
    "dev_cat_labels = list(categorize_prices(labels=dev_labels, thresholds = t))\n",
    "\n",
    "#use best method and parameters (above) to divide into categories\n",
    "#L1\n",
    "train_subset, dev_subset, nonzero_features = run_logr_l1(train_data, train_cat_labels, dev_data, final_features)\n",
    "#PCA with components = \n",
    "train_pca_subset, dev_pca_subset = run_pca(train_subset, dev_subset, components = c)\n",
    "#categorize (assume logR for now, could change)\n",
    "train_categories, dev_categories = run_logr_l2(train_pca_subset, train_cat_labels, dev_pca_subset, c=c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "The following blocks of code run various forms of regression on each category of data, and store the results for later analysis.\n",
    "In each case, we are running a grid search to optimize model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 96 105 113 125 133 137 142 154 187] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 96 103 107 113 120 125 133 137 142 148 154 159 167 187] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 96 103 105 108 109 113 115 125 133 137 142 154 183 187] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 96 105 113 114 115 125 133 137 142 144 145 154 159 171 187] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=20, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('lr', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False))])\n",
      "0.879443781914\n",
      "{'features__pca__n_components': 30, 'lr__fit_intercept': True, 'features__univ_select__k': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
      "  msw = sswn / float(dfwn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=15, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=10, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('lr', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False))])\n",
      "-0.310546736131\n",
      "{'features__pca__n_components': 15, 'lr__fit_intercept': True, 'features__univ_select__k': 10}\n"
     ]
    }
   ],
   "source": [
    "#build subsets of training and dev based on categories\n",
    "for cat in list(unique(train_categories)):\n",
    "    #print(cat)\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    idx_dev = list(np.array(np.where(dev_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    dev = dev_data.take(idx_dev, axis=0)  \n",
    "    dv_labels = dev_labels.take(idx_dev, axis=0)\n",
    "    \n",
    "    #for each category, run a gridsearch to determine optimal parameters for price prediction for category\n",
    "    pca = PCA(n_components=2) # PCA to remove demensionality and colinearity\n",
    "    selection = SelectKBest(k=1) # Select some original features where appropriate\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # Build estimator from PCA and Univariate selection:\n",
    "    features = combined_features.fit(train, trn_labels).transform(train) # Use combined features to transform dataset:\n",
    "    \n",
    "    #LR Regression\n",
    "    lr = LinearRegression(fit_intercept=True, n_jobs=-1)\n",
    "\n",
    "    # Do grid search over k, n_components and C:\n",
    "\n",
    "    pipeline = Pipeline([(\"features\", combined_features), (\"lr\", lr)])\n",
    "\n",
    "    param_grid = dict(features__pca__n_components=[10, 15, 20, 25, 30],\n",
    "                      features__univ_select__k=[5, 10, 15, 20],\n",
    "                      lr__fit_intercept=[True, False])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "    \n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    grid_results.append([cat,'lr',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=20, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       trans...timators=1000, n_jobs=-1, oob_score=False, random_state=0,\n",
      "           verbose=0, warm_start=False))])\n",
      "0.866877409008\n",
      "{'features__pca__n_components': 10, 'rfr__n_estimators': 1000, 'features__univ_select__k': 20}\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=20, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       trans...stimators=100, n_jobs=-1, oob_score=False, random_state=0,\n",
      "           verbose=0, warm_start=False))])\n",
      "-0.361159855131\n",
      "{'features__pca__n_components': 10, 'rfr__n_estimators': 100, 'features__univ_select__k': 20}\n"
     ]
    }
   ],
   "source": [
    "#build subsets of training and dev based on categories\n",
    "for cat in list(unique(train_categories)):\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    idx_dev = list(np.array(np.where(dev_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    dev = dev_data.take(idx_dev, axis=0)  \n",
    "    dv_labels = dev_labels.take(idx_dev, axis=0)\n",
    "    \n",
    "    #for each category, run a gridsearch to determine optimal parameters for price prediction for category\n",
    "    pca = PCA(n_components=2) # PCA to remove demensionality and colinearity\n",
    "    selection = SelectKBest(k=1) # Select some original features where appropriate\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # Build estimator from PCA and Univariate selection:\n",
    "    features = combined_features.fit(train, trn_labels).transform(train) # Use combined features to transform dataset:\n",
    "    \n",
    "    #Random Forest Regression\n",
    "    rfr = RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "\n",
    "    # Do grid search over k, n_components and # of estimators\n",
    "    pipeline = Pipeline([(\"features\", combined_features), (\"rfr\", rfr)])\n",
    "    param_grid = dict(features__pca__n_components=[10, 15, 20, 25, 30],\n",
    "                      features__univ_select__k=[5, 10, 15, 20],\n",
    "                      rfr__n_estimators=[100, 200, 500, 1000])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "    \n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results.append([cat,'rfr',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=1, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('knn', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=8, p=2,\n",
      "          weights='distance'))])\n",
      "0.812175086608\n",
      "{'features__pca__n_components': 10, 'knn__weights': 'distance', 'knn__n_neighbors': 8, 'knn__p': 2}\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=8, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=1, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('knn', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=2, p=2,\n",
      "          weights='uniform'))])\n",
      "0.0568503762614\n",
      "{'features__pca__n_components': 8, 'knn__weights': 'uniform', 'knn__n_neighbors': 2, 'knn__p': 2}\n"
     ]
    }
   ],
   "source": [
    "for cat in list(unique(train_categories)):\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    idx_dev = list(np.array(np.where(dev_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    dev = dev_data.take(idx_dev, axis=0)  \n",
    "    dv_labels = dev_labels.take(idx_dev, axis=0)\n",
    "    \n",
    "    #for each category, run a gridsearch to determine optimal parameters for price prediction for category\n",
    "    pca = PCA(n_components=2) # PCA to remove demensionality and colinearity\n",
    "    selection = SelectKBest(k=1) # Select some original features where appropriate\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # Build estimator from PCA and Univariate selection:\n",
    "    features = combined_features.fit(train, trn_labels).transform(train) # Use combined features to transform dataset:\n",
    "    \n",
    "    #KNN Regressor\n",
    "    knn = KNeighborsRegressor(n_neighbors=10, weights='uniform',n_jobs=-1, algorithm='auto')\n",
    "    pipeline = Pipeline([(\"features\", combined_features), (\"knn\", knn)])\n",
    "    param_grid = dict(features__pca__n_components=[2, 4, 6, 8, 10],\n",
    "                      #features__univ_select__k=[5, 10, 15, 20],\n",
    "                      knn__n_neighbors=[2, 4, 6, 8, 10],\n",
    "                      knn__weights=['uniform', 'distance'],\n",
    "                      knn__p=[1,2])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "\n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results.append([cat,'knn',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE LINEAR PERCEPTRON REGRESSION (NEURAL NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('mlp', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))])\n",
      "0.846379913328\n",
      "{'mlp__learning_rate': 'adaptive', 'mlp__alpha': 0.0001}\n",
      "Pipeline(steps=[('mlp', MLPRegressor(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))])\n",
      "-0.735828868865\n",
      "{'mlp__learning_rate': 'adaptive', 'mlp__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "for cat in list(unique(train_categories)):\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    #idx_dev = list(np.array(np.where(dev_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    #dev = dev_data.take(idx_dev, axis=0)  \n",
    "    #dv_labels = dev_labels.take(idx_dev, axis=0)\n",
    "    \n",
    "    mlp = MLPRegressor(alpha = 0.001, solver='lbfgs', learning_rate = 'constant')\n",
    "    \n",
    "    # Do grid search\n",
    "    pipeline = Pipeline([(\"mlp\", mlp)])\n",
    "    param_grid = dict(mlp__alpha = [0.0001, 0.001, .01, .1], \n",
    "                      mlp__learning_rate = ['constant', 'invscaling', 'adaptive'] )\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "\n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results.append([cat,'mlp',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=20, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('adr', AdaBoostRegressor(base_estimator=None, learning_rate=0.5, loss='exponential',\n",
      "         n_estimators=100, random_state=None))])\n",
      "0.818045971849\n",
      "{'features__pca__n_components': 30, 'adr__learning_rate': 0.5, 'adr__n_estimators': 100, 'adr__loss': 'exponential', 'features__univ_select__k': 20}\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=10, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('adr', AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='exponential',\n",
      "         n_estimators=50, random_state=None))])\n",
      "-0.389022348951\n",
      "{'features__pca__n_components': 20, 'adr__learning_rate': 0.1, 'adr__n_estimators': 50, 'adr__loss': 'exponential', 'features__univ_select__k': 10}\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Regression\n",
    "for cat in list(unique(train_categories)):\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    \n",
    "    #for each category, run a gridsearch to determine optimal parameters for price prediction for category\n",
    "    pca = PCA(n_components=2) # PCA to remove demensionality and colinearity\n",
    "    selection = SelectKBest(k=1) # Select some original features where appropriate\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # Build estimator from PCA and Univariate selection:\n",
    "    features = combined_features.fit(train, trn_labels).transform(train) # Use combined features to transform dataset:\n",
    "    \n",
    "    #adaboost Regression\n",
    "    adr = AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear')\n",
    "    #There is a trade-off between learning_rate and n_estimators\n",
    "    \n",
    "    pipeline = Pipeline([(\"features\", combined_features), (\"adr\", adr)])\n",
    "    param_grid = dict(features__pca__n_components=[10, 15, 20, 25, 30],\n",
    "                      features__univ_select__k=[5, 10, 15, 20],\n",
    "                      adr__n_estimators=[50, 100],\n",
    "                      adr__learning_rate=[.1, .5],\n",
    "                      adr__loss=['linear', 'square', 'exponential'])\n",
    "                      \n",
    "    # subset for rerun\n",
    "    # adr__n_estimators=[50, 100, 150, 200, 500, 1000]  \n",
    "    # adr__learning_rate=[.1, .5, 1, 1.5, 3]\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "\n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results.append([cat,'adb',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=20, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('svr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False))])\n",
      "0.206734757012\n",
      "{'features__pca__n_components': 30, 'svr__kernel': 'linear', 'svr__degree': 3, 'svr__C': 1.0, 'features__univ_select__k': 20}\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=15, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('univ_select', SelectKBest(k=5, score_func=<function f_classif at 0x000000000BBC1F98>))],\n",
      "       transformer_weights=None)), ('svr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False))])\n",
      "-0.0604063151747\n",
      "{'features__pca__n_components': 15, 'svr__kernel': 'poly', 'svr__degree': 3, 'svr__C': 1.0, 'features__univ_select__k': 5}\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regression\n",
    "for cat in list(unique(train_categories)):\n",
    "    #print(cat)\n",
    "    #get index of train and dev obs in this category, and separate data\n",
    "    idx = list(np.array(np.where(train_categories == cat))[0])\n",
    "    train = train_data.take(idx, axis=0) # axis = 0 provides the reduced components for positive\n",
    "    trn_labels = train_labels.take(idx, axis=0) \n",
    "    \n",
    "    #for each category, run a gridsearch to determine optimal parameters for price prediction for category\n",
    "    pca = PCA(n_components=2) # PCA to remove demensionality and colinearity\n",
    "    selection = SelectKBest(k=1) # Select some original features where appropriate\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # Build estimator from PCA and Univariate selection:\n",
    "    features = combined_features.fit(train, trn_labels).transform(train) # Use combined features to transform dataset:\n",
    "        \n",
    "    #Support Vector Regression\n",
    "    svr = SVR(C=1.0, kernel='rbf', degree=2, gamma='auto')\n",
    "    \n",
    "    # Do grid search over k, n_components and C:\n",
    "    pipeline = Pipeline([(\"features\", combined_features), (\"svr\", svr)])\n",
    "    param_grid = dict(features__pca__n_components=[10, 15, 20, 25, 30],\n",
    "                      features__univ_select__k=[5, 10, 15, 20],\n",
    "                      svr__C=[.1, 1.0],\n",
    "                      svr__kernel=['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "                      svr__degree=[2,3])               \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, refit=True)\n",
    "    grid_search.fit(train, trn_labels)\n",
    "    \n",
    "    # summarize the results of the grid search\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results.append([cat,'svr',grid_search]) # append into an array for post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PICKLE THE REGRESSION RESULTS FOR POST-PROCESSING AND SHARING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nONLY DO THIS ONCE\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ONLY DO THIS ONCE\n",
    "'''\n",
    "#pickle.dump(grid_results, open( \"grid_results.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SORT THE MODELS PER CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 lr 0.141171962208\n",
      "{'features__pca__n_components': 30, 'lr__fit_intercept': True, 'features__univ_select__k': 20}\n",
      "1 1 rfr 0.151791705895\n",
      "{'features__pca__n_components': 10, 'rfr__n_estimators': 1000, 'features__univ_select__k': 20}\n",
      "1 1 knn 0.248580605918\n",
      "{'features__pca__n_components': 10, 'knn__weights': 'distance', 'knn__n_neighbors': 8, 'knn__p': 2}\n",
      "1 1 mlp 0.183006141584\n",
      "{'mlp__learning_rate': 'adaptive', 'mlp__alpha': 0.0001}\n",
      "1 1 adb 0.175849811972\n",
      "{'features__pca__n_components': 30, 'adr__learning_rate': 0.5, 'adr__n_estimators': 100, 'adr__loss': 'exponential', 'features__univ_select__k': 20}\n",
      "1 1 svr 0.342505527237\n",
      "{'features__pca__n_components': 30, 'svr__kernel': 'linear', 'svr__degree': 3, 'svr__C': 1.0, 'features__univ_select__k': 20}\n",
      "2 2 lr 0.661059910508\n",
      "{'features__pca__n_components': 15, 'lr__fit_intercept': True, 'features__univ_select__k': 10}\n",
      "2 2 rfr 0.13143901443\n",
      "{'features__pca__n_components': 10, 'rfr__n_estimators': 100, 'features__univ_select__k': 20}\n",
      "2 2 knn 0.331717400769\n",
      "{'features__pca__n_components': 8, 'knn__weights': 'uniform', 'knn__n_neighbors': 2, 'knn__p': 2}\n",
      "2 2 mlp 0.42695448298\n",
      "{'mlp__learning_rate': 'adaptive', 'mlp__alpha': 0.1}\n",
      "2 2 adb 1.72214770443\n",
      "{'features__pca__n_components': 20, 'adr__learning_rate': 0.1, 'adr__n_estimators': 50, 'adr__loss': 'exponential', 'features__univ_select__k': 10}\n",
      "2 2 svr 0.327382703579\n",
      "{'features__pca__n_components': 15, 'svr__kernel': 'poly', 'svr__degree': 3, 'svr__C': 1.0, 'features__univ_select__k': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14084976459891552"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This code executes the best version of each model for each category\n",
    "\n",
    "Return model per category sorted by RMSLE performance\n",
    "'''\n",
    "best_cat_models = []\n",
    "for cat_num in [1,2]:\n",
    "    category_grid_results = [results for results in grid_results if results[0] == cat_num]\n",
    "    comparison_results = []\n",
    "    for result in category_grid_results:\n",
    "        category = result[0]\n",
    "        model_name = result[1]\n",
    "        best_params = result[2].best_params_\n",
    "\n",
    "        cat_train_data, cat_train_labels, cat_dev_data, cat_dev_labels = build_cat_data(category, model_name, \n",
    "                                                                                        best_params, train_data, dev_data,\n",
    "                                                                                       train_labels, dev_labels,\n",
    "                                                                                        train_categories, dev_categories)\n",
    "\n",
    "        #refit \n",
    "        if model_name == 'lr':\n",
    "            # initializing model with best parameters\n",
    "            model = LinearRegression(fit_intercept = best_params['lr__fit_intercept'])\n",
    "\n",
    "        elif model_name == 'rfr':\n",
    "            model = RandomForestRegressor(n_estimators=best_params['rfr__n_estimators'], random_state=0, n_jobs=-1)\n",
    "\n",
    "        elif model_name == 'mlp':\n",
    "            model = MLPRegressor(alpha = best_params['mlp__alpha'], solver='lbfgs', \n",
    "                                 learning_rate = best_params['mlp__learning_rate'])\n",
    "\n",
    "        elif model_name == 'svr':\n",
    "            model = SVR(C=best_params['svr__C'], kernel=best_params['svr__kernel'], degree=best_params['svr__degree'], gamma='auto')\n",
    "\n",
    "        elif model_name == 'adp':\n",
    "            model = AdaBoostRegressor(base_estimator=None, n_estimators=best_params['adr__n_estimators'], \n",
    "                                      learning_rate=best_params['adr__learning_rate'], loss=best_params['adr__loss'])\n",
    "\n",
    "        elif model_name == 'knn':\n",
    "            model = KNeighborsRegressor(n_neighbors=best_params['knn__n_neighbors'], \n",
    "                                        weights=best_params['knn__weights'], n_jobs=-1, \n",
    "                                        algorithm='auto', p=best_params['knn__p'])\n",
    "\n",
    "        # fitting and scoring model\n",
    "        model.fit(cat_train_data, cat_train_labels)\n",
    "        predictions = model.predict(cat_dev_data)\n",
    "        predictions[predictions < 0] = 0 # cheesy hack\n",
    "        model_score = rmsle(np.array(cat_dev_labels), predictions)\n",
    "        print cat_num, category, model_name, model_score\n",
    "        print best_params\n",
    "        # storing results as a 3-tuple for easy comparison and retrieving the model\n",
    "        comparison_results.append([model_name, model, result[2], model_score, predictions, cat_dev_labels])\n",
    "\n",
    "    best_cat_model = sorted(comparison_results, key=lambda x: x[3])[0]\n",
    "    best_cat_models.append(best_cat_model)\n",
    "\n",
    "pred1 = best_cat_models[0][4]\n",
    "actual1 = best_cat_models[0][5]\n",
    "pred2 = best_cat_models[1][4]\n",
    "actual2 = best_cat_models[1][5]\n",
    "\n",
    "pred = np.append(pred1,pred2)\n",
    "actual = np.append(actual1,actual2)\n",
    "\n",
    "score = rmsle(np.array(actual), pred)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results for Dev data\n",
    "Best RMSLE score for dev data: 0.1408\n",
    "\n",
    "This represents approximately 17% improvement in RMSLE score from our base model (0.17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFlCAYAAAB2nuuNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126ea908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the predicted vs actuals (similar to our original baseline model)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(actual, pred)\n",
    "ax.plot([dev_labels.min(), dev_labels.max()], [dev_labels.min(), dev_labels.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Labeled/Given')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.title(\"Dev Set (feature reduced and mulit-model) - Measured vs. Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to our baseline model (original chart from baseline file included again below as an image), the chart shows the most improvement at higher priced homes. \n",
    "Low and mid-tier homes has remained fairly consistent, however there is improvement in the clustering/bunching of observations along the line (as desired).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bslnchart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PICKLE IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(best_cat_models, open( \"best_cat_models.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING TEST DATA\n",
    "\n",
    "Note, test data could have been transformed and categorized along with training and dev originally.\n",
    "Team choose to separate the testing, and run at the end (keeping all of the testing processes together).\n",
    "For our process, this required re-running the optimal models at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORT and PROCESS TEST DATA\n",
    "\n",
    "# import test data\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# separate out Ids for submitting to Kaggle (similar to a label, but an idenitifier, vs price)\n",
    "test_ids = test['Id']\n",
    "\n",
    "# transform features\n",
    "# follows transformation used for training and dev data\n",
    "# note that is uses variables defined above and functions in the data_funcs.py file\n",
    "\n",
    "\n",
    "# transforming features\n",
    "test_data = test.set_index('Id')[keep_columns]\n",
    "test_data = transform_features(test_data,feature_dict,normal_cat_features,used_cat_features,\n",
    "                                likert_features,drop_dummies,drop_features)\n",
    "\n",
    "\n",
    "# Mean normalizing\n",
    "test_data = mean_normalization(test_data, features_to_normalize, mn_values)\n",
    "\n",
    "# convert to np arrays\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# deal with NaNs (which won't work in regression)\n",
    "where_are_NaNs = isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# APPLY CATEGORIZATION TO TEST DATA\n",
    "\n",
    "# categorize test data using best models identified\n",
    "t= [350000]         # cat threshold at 350,000\n",
    "pca_comp = 10    # 10 pca components\n",
    "smoothC = 10     # smoothing C at 10\n",
    "\n",
    "# categorize using best models identified\n",
    "train_cat_labels = list(categorize_prices(labels=train_labels, thresholds = t))\n",
    "train_subset, test_subset, nonzero_features = run_logr_l1(train_data, train_cat_labels, test_data, final_features)\n",
    "train_pca_subset, test_pca_subset = run_pca(train_subset, test_subset, components = pca_comp)\n",
    "train_categories, test_categories = run_logr_l2(train_pca_subset, train_cat_labels,test_pca_subset, c=smoothC)\n",
    "\n",
    "\n",
    "# run put 1421 test records under 350000 adn 38 above 350000 (in line with expectations)\n",
    "# len(test_categories[test_categories==0])\n",
    "# len(test_categories[test_categories==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLY REGRESSION TO TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# APPLY REGRESSION TO TEST DATA\n",
    "testids = []\n",
    "test_predictions = []\n",
    "\n",
    "for category_num in [0,1]:\n",
    "    model = best_cat_models[category_num]\n",
    "    model_name = model[0]\n",
    "    best_params = model[2].best_params_\n",
    "    \n",
    "    # reduce feature set\n",
    "    pca = PCA(n_components=best_params['features__pca__n_components'])\n",
    "        \n",
    "    # Select high value original features\n",
    "    selection = SelectKBest(k=best_params['features__univ_select__k'])\n",
    "        \n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "        \n",
    "    # Use combined features to transform dataset:\n",
    "    sub_features = combined_features.fit(train_data, train_labels)\n",
    "    train_reduced = sub_features.transform(train_data)\n",
    "    test_reduced = sub_features.transform(test_data)\n",
    "\n",
    "    \n",
    "    # separate out category data (1421 and 38 for test data)\n",
    "        \n",
    "    #subset out specific category\n",
    "    cat_train_data, cat_train_labels, cat_test_data, cat_test_ids = build_test_cats(category_num, train_reduced, test_reduced, \n",
    "                                                                      train_labels, test_ids, train_categories, test_categories)\n",
    "\n",
    "    # refit \n",
    "    if model_name == 'lr':\n",
    "        # initializing model with best parameters\n",
    "        model = LinearRegression(fit_intercept = best_params['lr__fit_intercept'])\n",
    "\n",
    "    elif model_name == 'rfr':\n",
    "        model = RandomForestRegressor(n_estimators=best_params['rfr__n_estimators'], random_state=0, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    model.fit(cat_train_data, cat_train_labels)\n",
    "    predictions = model.predict(cat_test_data)\n",
    "    predictions[predictions < 0] = 0 # hack for predictions less than zero\n",
    "    testids.append(cat_test_ids)\n",
    "    test_predictions.append(predictions)\n",
    "\n",
    "# flatten list of lists\n",
    "test_identifier = [val for sublist in testids for val in sublist]\n",
    "final_predictions = [val for sublist in test_predictions for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create csv to send to kaggle\n",
    "with open('mm_predictions_1.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('Id', 'SalePrice'))\n",
    "    writer.writerows(izip(test_identifier, final_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
